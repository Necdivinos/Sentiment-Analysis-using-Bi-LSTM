# -*- coding: utf-8 -*-
"""Assignment_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dCYspWT3fFHRXsxFANCI0b90UrAR70cU
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import pickle
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Download required NLTK data
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class RandomForestSentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.max_features = 10000
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.model_metrics = {}

    def preprocess_text(self, text):
        # Convert to lowercase
        text = text.lower()
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove user mentions and hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        # Remove special characters and digits, but keep basic punctuation
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Remove extra whitespaces
        text = re.sub(r'\s+', ' ', text).strip()

        # More advanced preprocessing - keep important words
        words = text.split()
        # Remove stopwords but keep negations and important sentiment words
        important_words = ['not', 'no', 'never', 'none', 'nothing', 'nobody', 'nowhere',
                          'neither', 'nor', 'barely', 'hardly', 'scarcely', 'seldom']
        words = [word for word in words if word not in self.stop_words or word in important_words]

        # Apply stemming
        words = [self.stemmer.stem(word) for word in words]

        return ' '.join(words)

    def create_pipeline(self, use_grid_search=False):
        # Create TF-IDF vectorizer
        vectorizer = TfidfVectorizer(
            max_features=self.max_features,
            ngram_range=(1, 2),  # Include both unigrams and bigrams
            min_df=2,
            max_df=0.95,
            strip_accents='unicode',
            lowercase=True,
            stop_words='english'
        )

        # Create Random Forest model
        rf_model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            n_jobs=-1,
            max_depth=20,
            min_samples_split=2,
            min_samples_leaf=1
        )

        # Create pipeline
        pipeline = Pipeline([
            ('vectorizer', vectorizer),
            ('model', rf_model)
        ])

        if use_grid_search:
            # Define parameter grid for optimization
            param_grid = {
                'vectorizer__max_features': [5000, 10000, 15000],
                'vectorizer__ngram_range': [(1, 1), (1, 2)],
                'model__n_estimators': [100, 200, 300],
                'model__max_depth': [10, 20, None],
                'model__min_samples_split': [2, 5, 10],
                'model__min_samples_leaf': [1, 2, 4]
            }

            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=3,
                scoring='roc_auc',
                n_jobs=-1,
                verbose=1
            )
            return grid_search

        return pipeline

    def train_model(self, df, use_grid_search=True):
        # Preprocess text data
        df['processed_text'] = df['reviewText'].apply(self.preprocess_text)

        # Split data with stratification
        X_train, X_test, y_train, y_test = train_test_split(
            df['processed_text'], df['Positive'],
            test_size=0.2, random_state=42, stratify=df['Positive']
        )

        # Further split training data for validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )

        # Create and train model
        self.model = self.create_pipeline(use_grid_search)

        # Train model
        self.model.fit(X_train, y_train)

        # Get best parameters if grid search was used
        best_params = {}
        if use_grid_search:
            best_params = self.model.best_params_
            self.model = self.model.best_estimator_

        # Make predictions
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]  # Probability of positive class
        y_pred = self.model.predict(X_test)

        # Make validation predictions
        y_val_pred_proba = self.model.predict_proba(X_val)[:, 1]
        y_val_pred = self.model.predict(X_val)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        val_accuracy = accuracy_score(y_val, y_val_pred)
        auc_score = roc_auc_score(y_test, y_pred_proba)
        val_auc_score = roc_auc_score(y_val, y_val_pred_proba)

        # Get detailed metrics
        report = classification_report(y_test, y_pred, output_dict=True)

        # Feature importance
        feature_importance = None
        try:
            feature_names = self.model.named_steps['vectorizer'].get_feature_names_out()
            importance_scores = self.model.named_steps['model'].feature_importances_
            feature_importance = dict(zip(feature_names, importance_scores))
            # Get top 20 features
            feature_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:20])
        except Exception as e:
            st.warning(f"Could not extract feature importance: {e}")

        self.model_metrics = {
            'model_name': 'Random Forest',
            'hyperparameters': best_params,
            'accuracy': accuracy,
            'val_accuracy': val_accuracy,
            'auc_score': auc_score,
            'val_auc_score': val_auc_score,
            'classification_report': report,
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'feature_importance': feature_importance,
            'X_test': X_test,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'X_val': X_val,
            'y_val': y_val,
            'y_val_pred_proba': y_val_pred_proba,
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test)
        }

        return self.model_metrics

    def predict(self, text):
        if self.model is None:
            return None, None

        # Preprocess text
        processed_text = self.preprocess_text(text)

        # Make prediction
        prediction_proba = self.model.predict_proba([processed_text])[0]
        prediction = self.model.predict([processed_text])[0]

        return prediction, prediction_proba

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('amazon.csv')
        # Data quality checks and improvements
        df = df.dropna(subset=['reviewText', 'Positive'])
        df['reviewText'] = df['reviewText'].astype(str)

        # Balance the dataset if heavily imbalanced
        pos_count = df['Positive'].sum()
        neg_count = len(df) - pos_count

        if abs(pos_count - neg_count) > min(pos_count, neg_count) * 0.5:
            # If imbalanced, sample equal amounts
            df_pos = df[df['Positive'] == 1].sample(min(pos_count, neg_count, 5000), random_state=42)
            df_neg = df[df['Positive'] == 0].sample(min(pos_count, neg_count, 5000), random_state=42)
            df = pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)

        return df
    except FileNotFoundError:
        st.error("amazon.csv file not found. Please make sure the file is in the same directory.")
        return None

@st.cache_resource
def train_model(df, use_grid_search):
    analyzer = RandomForestSentimentAnalyzer()
    metrics = analyzer.train_model(df, use_grid_search)
    return analyzer, metrics

def plot_feature_importance(feature_importance):
    """Plot top feature importance"""
    if feature_importance is None:
        return None

    features = list(feature_importance.keys())
    importance = list(feature_importance.values())

    fig, ax = plt.subplots(figsize=(10, 8))
    bars = ax.barh(features, importance, color='lightgreen', alpha=0.8)
    ax.set_title('Top 20 Most Important Features (Random Forest)', fontweight='bold', fontsize=14)
    ax.set_xlabel('Feature Importance', fontsize=12)

    # Add value labels
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width + max(importance) * 0.01, bar.get_y() + bar.get_height()/2.,
               f'{width:.4f}', ha='left', va='center', fontweight='bold')

    plt.tight_layout()
    return fig

def plot_validation_metrics(metrics):
    """Plot training vs validation metrics comparison"""
    categories = ['Accuracy', 'AUC Score']
    train_scores = [metrics['accuracy'], metrics['auc_score']]
    val_scores = [metrics['val_accuracy'], metrics['val_auc_score']]

    x = np.arange(len(categories))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    bars1 = ax.bar(x - width/2, train_scores, width, label='Test Set', color='skyblue', alpha=0.8)
    bars2 = ax.bar(x + width/2, val_scores, width, label='Validation Set', color='lightcoral', alpha=0.8)

    ax.set_xlabel('Metrics', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Model Performance: Test vs Validation', fontweight='bold', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(categories)
    ax.legend()
    ax.set_ylim(0, 1)

    # Add value labels on bars
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    return fig

def main():
    st.set_page_config(page_title="Random Forest Sentiment Analysis Dashboard", layout="wide")
    st.title("ğŸŒ² Random Forest Sentiment Analysis Dashboard")
    st.markdown("*Using Random Forest classifier with TF-IDF features instead of deep learning*")

    # Sidebar for configuration
    st.sidebar.header("ğŸ”§ Model Configuration")

    use_grid_search = st.sidebar.checkbox(
        "Enable Hyperparameter Tuning (Grid Search)",
        value=True,
        help="This will optimize model parameters but may take longer to train"
    )

    if use_grid_search:
        st.sidebar.info("ğŸ” Grid Search will optimize:\n- TF-IDF parameters\n- Random Forest parameters\n- This may take 2-5 minutes")

    # Load data
    df = load_data()
    if df is None:
        return

    st.success(f"âœ… Dataset loaded successfully! Total samples: {len(df)}")

    # Show data distribution
    pos_count = df['Positive'].sum()
    neg_count = len(df) - pos_count
    st.info(f"ğŸ“Š Data Distribution: {pos_count} Positive ({pos_count/len(df)*100:.1f}%) | {neg_count} Negative ({neg_count/len(df)*100:.1f}%)")

    # Train model
    with st.spinner("ğŸŒ² Training Random Forest model... This may take a few minutes."):
        analyzer, metrics = train_model(df, use_grid_search)

    st.success("âœ… Model training completed!")

    # Create tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š Model Performance", "ğŸ” Feature Analysis", "ğŸ“ˆ Model Validation", "ğŸ”® Live Prediction"])

    with tab1:
        st.header("ğŸ¯ Random Forest Model Performance")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ğŸ¯ Accuracy", f"{metrics['accuracy']:.4f}", delta=f"+{(metrics['accuracy']-0.5)*100:.1f}% vs Random")
            st.metric("ğŸ“ˆ AUC Score", f"{metrics['auc_score']:.4f}")

        with col2:
            precision = metrics['classification_report']['weighted avg']['precision']
            recall = metrics['classification_report']['weighted avg']['recall']
            f1 = metrics['classification_report']['weighted avg']['f1-score']

            st.metric("ğŸ¯ Precision", f"{precision:.4f}")
            st.metric("ğŸ”„ Recall", f"{recall:.4f}")

        with col3:
            st.metric("âš¡ F1-Score", f"{f1:.4f}")
            st.metric("ğŸ”¢ Test Samples", f"{metrics['test_size']:,}")

        st.subheader("ğŸ“‹ Detailed Model Information")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**ğŸŒ² Model Architecture:**")
            st.write(f"- Model Type: {metrics['model_name']}")
            st.write(f"- Feature Extraction: TF-IDF Vectorization")
            st.write(f"- N-gram Range: Unigrams + Bigrams")
            st.write(f"- Max Features: {analyzer.max_features:,}")
            st.write(f"- Preprocessing: Stemming + Stopword Removal")

            st.write("**ğŸ“Š Dataset Split:**")
            st.write(f"- Training: {metrics['train_size']:,} samples")
            st.write(f"- Validation: {metrics['val_size']:,} samples")
            st.write(f"- Testing: {metrics['test_size']:,} samples")

            if metrics['hyperparameters']:
                st.write("**ğŸ”§ Best Parameters (Grid Search):**")
                for param, value in metrics['hyperparameters'].items():
                    st.write(f"- {param}: {value}")

        with col2:
            st.subheader("ğŸ“ˆ Classification Report")
            report_df = pd.DataFrame(metrics['classification_report']).transpose()
            st.dataframe(report_df.round(4), use_container_width=True)

        # Visualizations
        st.subheader("ğŸ“Š Performance Visualizations")

        col1, col2 = st.columns(2)

        with col1:
            # Confusion Matrix
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(metrics['confusion_matrix'],
                       annot=True,
                       fmt='d',
                       cmap='Greens',
                       xticklabels=['Negative', 'Positive'],
                       yticklabels=['Negative', 'Positive'],
                       cbar_kws={'label': 'Count'})
            plt.title('ğŸ¯ Confusion Matrix', fontsize=14, fontweight='bold')
            plt.ylabel('Actual Label', fontsize=12)
            plt.xlabel('Predicted Label', fontsize=12)
            st.pyplot(fig)

        with col2:
            # ROC Curve
            fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'])
            fig, ax = plt.subplots(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkgreen', lw=3,
                    label=f'ROC Curve (AUC = {metrics["auc_score"]:.4f})')
            plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8)
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate', fontsize=12)
            plt.ylabel('True Positive Rate', fontsize=12)
            plt.title('ğŸ“ˆ ROC Curve', fontsize=14, fontweight='bold')
            plt.legend(loc="lower right")
            plt.grid(True, alpha=0.3)
            st.pyplot(fig)

    with tab2:
        st.header("ğŸ” Feature Importance Analysis")

        if metrics['feature_importance']:
            st.subheader("ğŸ† Top 20 Most Important Features")

            fig = plot_feature_importance(metrics['feature_importance'])
            if fig:
                st.pyplot(fig)

            st.subheader("ğŸ“Š Feature Importance Details")
            importance_df = pd.DataFrame(
                list(metrics['feature_importance'].items()),
                columns=['Feature', 'Importance']
            ).sort_values('Importance', ascending=False)

            st.dataframe(importance_df, use_container_width=True)

            st.info("ğŸ’¡ **Interpretation:** Higher importance scores indicate features that contribute more to the model's decision-making process. These are typically words or phrases that strongly correlate with positive or negative sentiment.")
        else:
            st.warning("Feature importance data not available.")

    with tab3:
        st.header("ğŸ“ˆ Model Validation Analysis")

        st.subheader("ğŸ”„ Training vs Validation Performance")
        fig = plot_validation_metrics(metrics)
        st.pyplot(fig)

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ğŸ“Š Performance Summary")
            st.write("**Test Set Performance:**")
            st.write(f"- Accuracy: {metrics['accuracy']:.4f}")
            st.write(f"- AUC Score: {metrics['auc_score']:.4f}")

            st.write("**Validation Set Performance:**")
            st.write(f"- Accuracy: {metrics['val_accuracy']:.4f}")
            st.write(f"- AUC Score: {metrics['val_auc_score']:.4f}")

        with col2:
            st.subheader("ğŸ¯ Model Assessment")

            acc_diff = abs(metrics['accuracy'] - metrics['val_accuracy'])
            auc_diff = abs(metrics['auc_score'] - metrics['val_auc_score'])

            if acc_diff < 0.02 and auc_diff < 0.02:
                st.success("âœ… **Excellent Generalization**: The model performs consistently across test and validation sets.")
            elif acc_diff < 0.05 and auc_diff < 0.05:
                st.info("âœ… **Good Generalization**: Minor differences between test and validation performance.")
            else:
                st.warning("âš ï¸ **Potential Overfitting**: Significant difference between test and validation performance detected.")

            st.write(f"Accuracy Difference: {acc_diff:.4f}")
            st.write(f"AUC Difference: {auc_diff:.4f}")

    with tab4:
        st.header("ğŸ”® Live Sentiment Prediction")
        st.write("Enter any text below to analyze its sentiment using our Random Forest model:")

        # Predefined examples
        st.subheader("ğŸ’¡ Try these examples:")
        examples = [
            "This product is absolutely amazing! I love it so much!",
            "Terrible quality, waste of money. Very disappointed.",
            "It's okay, nothing special but does the job.",
            "Outstanding customer service and fast delivery. Highly recommended!",
            "The product broke after just one day. Poor quality control."
        ]

        example_cols = st.columns(len(examples))
        for i, example in enumerate(examples):
            if example_cols[i].button(f"Example {i+1}", key=f"example_{i}"):
                st.session_state['example_text'] = example

        # Text input
        user_text = st.text_area(
            "Enter text for sentiment analysis:",
            value=st.session_state.get('example_text', ''),
            placeholder="Type or paste your text here... (e.g., product reviews, tweets, comments)",
            height=120,
            key="user_input"
        )

        col1, col2 = st.columns([1, 4])
        with col1:
            analyze_button = st.button("ğŸ” Analyze Sentiment", type="primary", use_container_width=True)

        if analyze_button and user_text.strip():
            with st.spinner("ğŸŒ² Analyzing sentiment with Random Forest..."):
                prediction, probabilities = analyzer.predict(user_text)

                if prediction is not None:
                    # Display results with enhanced styling
                    st.subheader("ğŸ“Š Analysis Results")

                    col1, col2, col3 = st.columns([2, 1, 2])

                    with col1:
                        sentiment_label = "Positive ğŸ˜Š" if prediction == 1 else "Negative ğŸ˜"
                        sentiment_color = "green" if prediction == 1 else "red"
                        confidence = max(probabilities) * 100

                        st.markdown(f"""
                        <div style='text-align: center; padding: 20px; border-radius: 10px; background-color: rgba({255 if prediction == 0 else 0}, {255 if prediction == 1 else 0}, 0, 0.1); border: 2px solid {sentiment_color}'>
                            <h2 style='color: {sentiment_color}; margin-bottom: 10px'>
                                ğŸŒ² {sentiment_label}
                            </h2>
                            <p style='font-size: 18px; margin: 0'>
                                Confidence: <strong>{confidence:.1f}%</strong>
                            </p>
                        </div>
                        """, unsafe_allow_html=True)

                    with col2:
                        st.write("")  # Spacer

                    with col3:
                        st.subheader("ğŸ“ˆ Confidence Breakdown")
                        negative_conf = probabilities[0] * 100
                        positive_conf = probabilities[1] * 100

                        # Enhanced progress bars
                        st.metric("Negative Score", f"{negative_conf:.2f}%")
                        st.progress(negative_conf/100, text=f"ğŸ”´ {negative_conf:.1f}%")

                        st.metric("Positive Score", f"{positive_conf:.2f}%")
                        st.progress(positive_conf/100, text=f"ğŸŸ¢ {positive_conf:.1f}%")

                    # Enhanced confidence visualization
                    st.subheader("ğŸ“Š Detailed Confidence Visualization")
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

                    # Bar chart
                    labels = ['Negative ğŸ˜', 'Positive ğŸ˜Š']
                    colors = ['#ff6b6b', '#51cf66']
                    bars = ax1.bar(labels, [negative_conf, positive_conf], color=colors, alpha=0.8, edgecolor='white', linewidth=2)
                    ax1.set_ylabel('Confidence (%)', fontsize=12)
                    ax1.set_title('Random Forest Sentiment Confidence', fontsize=14, fontweight='bold')
                    ax1.set_ylim(0, 100)
                    ax1.grid(True, alpha=0.3)

                    # Add value labels on bars
                    for bar in bars:
                        height = bar.get_height()
                        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,
                               f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

                    # Pie chart
                    wedges, texts, autotexts = ax2.pie([negative_conf, positive_conf],
                                                      labels=labels,
                                                      colors=colors,
                                                      autopct='%1.1f%%',
                                                      startangle=90,
                                                      textprops={'fontsize': 11, 'fontweight': 'bold'})
                    ax2.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')

                    plt.tight_layout()
                    st.pyplot(fig)

                    # Interpretation
                    st.subheader("ğŸ¯ Interpretation")
                    if confidence > 80:
                        st.success(f"ğŸ¯ **High Confidence**: The Random Forest model is very confident that this text expresses {sentiment_label.split()[1]} sentiment.")
                    elif confidence > 60:
                        st.info(f"âœ… **Moderate Confidence**: The model believes this text is {sentiment_label.split()[1]}, but with moderate confidence.")
                    else:
                        st.warning(f"âš ï¸ **Low Confidence**: The model leans towards {sentiment_label.split()[1]} sentiment, but the confidence is relatively low. The text might be neutral or ambiguous.")

                    # Show preprocessing result
                    with st.expander("ğŸ” View Preprocessing Details"):
                        processed_text = analyzer.preprocess_text(user_text)
                        st.write("**Original Text:**")
                        st.write(user_text)
                        st.write("**Processed Text (after cleaning, stemming, stopword removal):**")
                        st.write(processed_text)

                else:
                    st.error("âŒ Error in prediction. Please try again.")
        elif analyze_button:
            st.warning("âš ï¸ Please enter some text to analyze.")

    # Footer
    st.markdown("---")
    st.markdown("**ğŸŒ² Random Forest Sentiment Analysis** - Built with scikit-learn and Streamlit")
    st.markdown("*No deep learning frameworks required!*")

if __name__ == "__main__":
    main()