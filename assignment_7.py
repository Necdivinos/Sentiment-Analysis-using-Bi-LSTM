# -*- coding: utf-8 -*-
"""Assignment_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dCYspWT3fFHRXsxFANCI0b90UrAR70cU
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import pickle
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Download required NLTK data
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class LSTMSentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.max_features = 10000
        self.max_length = 100
        self.embedding_dim = 128
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.model_metrics = {}

    def preprocess_text(self, text):
        # Convert to lowercase
        text = text.lower()
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove user mentions and hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        # Remove special characters and digits, but keep basic punctuation
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Remove extra whitespaces
        text = re.sub(r'\s+', ' ', text).strip()

        # More advanced preprocessing - keep important words
        words = text.split()
        # Remove stopwords but keep negations and important sentiment words
        important_words = ['not', 'no', 'never', 'none', 'nothing', 'nobody', 'nowhere',
                          'neither', 'nor', 'barely', 'hardly', 'scarcely', 'seldom']
        words = [word for word in words if word not in self.stop_words or word in important_words]

        return ' '.join(words)

    def create_model(self):
        model = Sequential([
            Embedding(input_dim=self.max_features,
                     output_dim=self.embedding_dim,
                     input_length=self.max_length,
                     mask_zero=True),
            Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),
            Bidirectional(LSTM(32, dropout=0.3, recurrent_dropout=0.3)),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )

        return model

    def train_model(self, df):
        # Preprocess text data
        df['processed_text'] = df['reviewText'].apply(self.preprocess_text)

        # Split data with stratification
        X_train, X_test, y_train, y_test = train_test_split(
            df['processed_text'], df['Positive'],
            test_size=0.2, random_state=42, stratify=df['Positive']
        )

        # Further split training data for validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )

        # Tokenize text data
        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token='<OOV>')
        self.tokenizer.fit_on_texts(X_train)

        # Convert texts to sequences
        X_train_seq = self.tokenizer.texts_to_sequences(X_train)
        X_val_seq = self.tokenizer.texts_to_sequences(X_val)
        X_test_seq = self.tokenizer.texts_to_sequences(X_test)

        # Pad sequences
        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_length, padding='post')
        X_val_pad = pad_sequences(X_val_seq, maxlen=self.max_length, padding='post')
        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_length, padding='post')

        # Create and compile model
        self.model = self.create_model()

        # Define callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )

        lr_reduction = ReduceLROnPlateau(
            monitor='val_loss',
            patience=3,
            factor=0.5,
            min_lr=1e-7,
            verbose=1
        )

        # Train model
        history = self.model.fit(
            X_train_pad, y_train,
            batch_size=32,
            epochs=20,
            validation_data=(X_val_pad, y_val),
            callbacks=[early_stopping, lr_reduction],
            verbose=1
        )

        # Make predictions
        y_pred_proba = self.model.predict(X_test_pad, verbose=0)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        auc_score = roc_auc_score(y_test, y_pred_proba)

        # Get detailed metrics
        report = classification_report(y_test, y_pred, output_dict=True)

        self.model_metrics = {
            'model_name': 'Bidirectional LSTM',
            'hyperparameters': {
                'max_features': self.max_features,
                'max_length': self.max_length,
                'embedding_dim': self.embedding_dim,
                'lstm_units': [64, 32],
                'dropout': [0.3, 0.5, 0.3],
                'optimizer': 'Adam',
                'learning_rate': 0.001
            },
            'accuracy': accuracy,
            'auc_score': auc_score,
            'classification_report': report,
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'X_test': X_test,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba.flatten(),
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'history': history
        }

        return self.model_metrics

    def predict(self, text):
        if self.model is None or self.tokenizer is None:
            return None, None

        # Preprocess text
        processed_text = self.preprocess_text(text)

        # Convert to sequence and pad
        sequence = self.tokenizer.texts_to_sequences([processed_text])
        padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post')

        # Make prediction
        prediction_proba = self.model.predict(padded_sequence, verbose=0)[0][0]
        prediction = 1 if prediction_proba > 0.5 else 0

        # Return probabilities for both classes
        probabilities = np.array([1 - prediction_proba, prediction_proba])

        return prediction, probabilities

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('amazon.csv')
        # Data quality checks and improvements
        df = df.dropna(subset=['reviewText', 'Positive'])
        df['reviewText'] = df['reviewText'].astype(str)

        # Balance the dataset if heavily imbalanced
        pos_count = df['Positive'].sum()
        neg_count = len(df) - pos_count

        if abs(pos_count - neg_count) > min(pos_count, neg_count) * 0.5:
            # If imbalanced, sample equal amounts
            df_pos = df[df['Positive'] == 1].sample(min(pos_count, neg_count, 5000), random_state=42)
            df_neg = df[df['Positive'] == 0].sample(min(pos_count, neg_count, 5000), random_state=42)
            df = pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)

        return df
    except FileNotFoundError:
        st.error("amazon.csv file not found. Please make sure the file is in the same directory.")
        return None

@st.cache_resource
def train_model(df):
    analyzer = LSTMSentimentAnalyzer()
    metrics = analyzer.train_model(df)
    return analyzer, metrics

def plot_training_history(history):
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

    # Accuracy
    ax1.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')
    ax1.set_title('Model Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)

    # Loss
    ax2.plot(history.history['loss'], label='Training Loss', color='blue')
    ax2.plot(history.history['val_loss'], label='Validation Loss', color='red')
    ax2.set_title('Model Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True)

    # Precision
    ax3.plot(history.history['precision'], label='Training Precision', color='blue')
    ax3.plot(history.history['val_precision'], label='Validation Precision', color='red')
    ax3.set_title('Model Precision')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Precision')
    ax3.legend()
    ax3.grid(True)

    # Recall
    ax4.plot(history.history['recall'], label='Training Recall', color='blue')
    ax4.plot(history.history['val_recall'], label='Validation Recall', color='red')
    ax4.set_title('Model Recall')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Recall')
    ax4.legend()
    ax4.grid(True)

    plt.tight_layout()
    return fig

def main():
    st.set_page_config(page_title="Advanced Sentiment Analysis Dashboard", layout="wide")
    st.title("üöÄ Advanced LSTM Sentiment Analysis Dashboard")

    # Load data
    df = load_data()
    if df is None:
        return

    st.success(f"‚úÖ Dataset loaded successfully! Total samples: {len(df)}")

    # Show data distribution
    pos_count = df['Positive'].sum()
    neg_count = len(df) - pos_count
    st.info(f"üìä Data Distribution: {pos_count} Positive ({pos_count/len(df)*100:.1f}%) | {neg_count} Negative ({neg_count/len(df)*100:.1f}%)")

    # Train model
    with st.spinner("üîÑ Training advanced LSTM model... This may take a few minutes."):
        analyzer, metrics = train_model(df)

    st.success("‚úÖ Model training completed!")

    # Create tabs
    tab1, tab2, tab3 = st.tabs(["üìä Model Performance", "üìà Training History", "üîÆ Live Prediction"])

    with tab1:
        st.header("üéØ Model Performance Metrics")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("üéØ Accuracy", f"{metrics['accuracy']:.4f}", delta=f"+{(metrics['accuracy']-0.5)*100:.1f}% vs Random")
            st.metric("üìà AUC Score", f"{metrics['auc_score']:.4f}")

        with col2:
            precision = metrics['classification_report']['weighted avg']['precision']
            recall = metrics['classification_report']['weighted avg']['recall']
            f1 = metrics['classification_report']['weighted avg']['f1-score']

            st.metric("üéØ Precision", f"{precision:.4f}")
            st.metric("üîÑ Recall", f"{recall:.4f}")

        with col3:
            st.metric("‚ö° F1-Score", f"{f1:.4f}")
            st.metric("üî¢ Test Samples", f"{metrics['test_size']:,}")

        st.subheader("üìã Detailed Model Information")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**üèóÔ∏è Model Architecture:**")
            st.write(f"- Model Type: {metrics['model_name']}")
            st.write(f"- Max Features: {metrics['hyperparameters']['max_features']:,}")
            st.write(f"- Sequence Length: {metrics['hyperparameters']['max_length']}")
            st.write(f"- Embedding Dimension: {metrics['hyperparameters']['embedding_dim']}")
            st.write(f"- LSTM Units: {metrics['hyperparameters']['lstm_units']}")
            st.write(f"- Dropout Rates: {metrics['hyperparameters']['dropout']}")

            st.write("**üìä Dataset Split:**")
            st.write(f"- Training: {metrics['train_size']:,} samples")
            st.write(f"- Validation: {metrics['val_size']:,} samples")
            st.write(f"- Testing: {metrics['test_size']:,} samples")

        with col2:
            st.subheader("üìà Classification Report")
            report_df = pd.DataFrame(metrics['classification_report']).transpose()
            st.dataframe(report_df.round(4), use_container_width=True)

        # Visualizations
        st.subheader("üìä Performance Visualizations")

        col1, col2 = st.columns(2)

        with col1:
            # Confusion Matrix
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(metrics['confusion_matrix'],
                       annot=True,
                       fmt='d',
                       cmap='Blues',
                       xticklabels=['Negative', 'Positive'],
                       yticklabels=['Negative', 'Positive'],
                       cbar_kws={'label': 'Count'})
            plt.title('üéØ Confusion Matrix', fontsize=14, fontweight='bold')
            plt.ylabel('Actual Label', fontsize=12)
            plt.xlabel('Predicted Label', fontsize=12)
            st.pyplot(fig)

        with col2:
            # ROC Curve
            fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'])
            fig, ax = plt.subplots(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=3,
                    label=f'ROC Curve (AUC = {metrics["auc_score"]:.4f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate', fontsize=12)
            plt.ylabel('True Positive Rate', fontsize=12)
            plt.title('üìà ROC Curve', fontsize=14, fontweight='bold')
            plt.legend(loc="lower right")
            plt.grid(True, alpha=0.3)
            st.pyplot(fig)

    with tab2:
        st.header("üìà Training Progress")

        if 'history' in metrics and metrics['history']:
            fig = plot_training_history(metrics['history'])
            st.pyplot(fig)

            st.subheader("üìä Training Summary")
            col1, col2, col3 = st.columns(3)

            with col1:
                final_train_acc = metrics['history'].history['accuracy'][-1]
                final_val_acc = metrics['history'].history['val_accuracy'][-1]
                st.metric("Final Training Accuracy", f"{final_train_acc:.4f}")
                st.metric("Final Validation Accuracy", f"{final_val_acc:.4f}")

            with col2:
                final_train_loss = metrics['history'].history['loss'][-1]
                final_val_loss = metrics['history'].history['val_loss'][-1]
                st.metric("Final Training Loss", f"{final_train_loss:.4f}")
                st.metric("Final Validation Loss", f"{final_val_loss:.4f}")

            with col3:
                epochs_trained = len(metrics['history'].history['loss'])
                overfitting_score = abs(final_train_acc - final_val_acc)
                st.metric("Epochs Trained", f"{epochs_trained}")
                st.metric("Overfitting Score", f"{overfitting_score:.4f}")

                if overfitting_score < 0.05:
                    st.success("‚úÖ Good generalization!")
                elif overfitting_score < 0.1:
                    st.warning("‚ö†Ô∏è Slight overfitting")
                else:
                    st.error("‚ùå Significant overfitting detected")
        else:
            st.error("Training history not available.")

    with tab3:
        st.header("üîÆ Live Sentiment Prediction")
        st.write("Enter any text below to analyze its sentiment using our advanced LSTM model:")

        # Predefined examples
        st.subheader("üí° Try these examples:")
        examples = [
            "This product is absolutely amazing! I love it so much!",
            "Terrible quality, waste of money. Very disappointed.",
            "It's okay, nothing special but does the job.",
            "Outstanding customer service and fast delivery. Highly recommended!",
            "The product broke after just one day. Poor quality control."
        ]

        example_cols = st.columns(len(examples))
        for i, example in enumerate(examples):
            if example_cols[i].button(f"Example {i+1}", key=f"example_{i}"):
                st.session_state['example_text'] = example

        # Text input
        user_text = st.text_area(
            "Enter text for sentiment analysis:",
            value=st.session_state.get('example_text', ''),
            placeholder="Type or paste your text here... (e.g., product reviews, tweets, comments)",
            height=120,
            key="user_input"
        )

        col1, col2 = st.columns([1, 4])
        with col1:
            analyze_button = st.button("üîç Analyze Sentiment", type="primary", use_container_width=True)

        if analyze_button and user_text.strip():
            with st.spinner("üß† Analyzing sentiment..."):
                prediction, probabilities = analyzer.predict(user_text)

                if prediction is not None:
                    # Display results with enhanced styling
                    st.subheader("üìä Analysis Results")

                    col1, col2, col3 = st.columns([2, 1, 2])

                    with col1:
                        sentiment_label = "Positive üòä" if prediction == 1 else "Negative üòû"
                        sentiment_color = "green" if prediction == 1 else "red"
                        confidence = max(probabilities) * 100

                        st.markdown(f"""
                        <div style='text-align: center; padding: 20px; border-radius: 10px; background-color: rgba({255 if prediction == 0 else 0}, {255 if prediction == 1 else 0}, 0, 0.1); border: 2px solid {sentiment_color}'>
                            <h2 style='color: {sentiment_color}; margin-bottom: 10px'>
                                {sentiment_label}
                            </h2>
                            <p style='font-size: 18px; margin: 0'>
                                Confidence: <strong>{confidence:.1f}%</strong>
                            </p>
                        </div>
                        """, unsafe_allow_html=True)

                    with col2:
                        st.write("")  # Spacer

                    with col3:
                        st.subheader("üìà Confidence Breakdown")
                        negative_conf = probabilities[0] * 100
                        positive_conf = probabilities[1] * 100

                        # Enhanced progress bars
                        st.metric("Negative Score", f"{negative_conf:.2f}%")
                        st.progress(negative_conf/100, text=f"üî¥ {negative_conf:.1f}%")

                        st.metric("Positive Score", f"{positive_conf:.2f}%")
                        st.progress(positive_conf/100, text=f"üü¢ {positive_conf:.1f}%")

                    # Enhanced confidence visualization
                    st.subheader("üìä Detailed Confidence Visualization")
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

                    # Bar chart
                    labels = ['Negative üòû', 'Positive üòä']
                    colors = ['#ff6b6b', '#51cf66']
                    bars = ax1.bar(labels, [negative_conf, positive_conf], color=colors, alpha=0.8, edgecolor='white', linewidth=2)
                    ax1.set_ylabel('Confidence (%)', fontsize=12)
                    ax1.set_title('Sentiment Confidence Levels', fontsize=14, fontweight='bold')
                    ax1.set_ylim(0, 100)
                    ax1.grid(True, alpha=0.3)

                    # Add value labels on bars
                    for bar in bars:
                        height = bar.get_height()
                        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,
                               f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

                    # Pie chart
                    wedges, texts, autotexts = ax2.pie([negative_conf, positive_conf],
                                                      labels=labels,
                                                      colors=colors,
                                                      autopct='%1.1f%%',
                                                      startangle=90,
                                                      textprops={'fontsize': 11, 'fontweight': 'bold'})
                    ax2.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')

                    plt.tight_layout()
                    st.pyplot(fig)

                    # Interpretation
                    st.subheader("üéØ Interpretation")
                    if confidence > 80:
                        st.success(f"üéØ **High Confidence**: The model is very confident that this text expresses {sentiment_label.split()[0].lower()} sentiment.")
                    elif confidence > 60:
                        st.info(f"‚úÖ **Moderate Confidence**: The model believes this text is {sentiment_label.split()[0].lower()}, but with moderate confidence.")
                    else:
                        st.warning(f"‚ö†Ô∏è **Low Confidence**: The model leans towards {sentiment_label.split()[0].lower()} sentiment, but the confidence is relatively low. The text might be neutral or ambiguous.")

                else:
                    st.error("‚ùå Error in prediction. Please try again.")
        elif analyze_button:
            st.warning("‚ö†Ô∏è Please enter some text to analyze.")

if __name__ == "__main__":
    main()