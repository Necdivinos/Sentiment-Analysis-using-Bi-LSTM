# -*- coding: utf-8 -*-
"""Assignment_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dCYspWT3fFHRXsxFANCI0b90UrAR70cU
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import pickle
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class SentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.model_metrics = {}

    def preprocess_text(self, text):
        # Convert to lowercase
        text = text.lower()
        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Remove extra whitespaces
        text = re.sub(r'\s+', ' ', text).strip()
        # Remove stopwords and apply stemming
        words = text.split()
        words = [self.stemmer.stem(word) for word in words if word not in self.stop_words]
        return ' '.join(words)

    def train_model(self, df):
        # Preprocess text data
        df['processed_text'] = df['reviewText'].apply(self.preprocess_text)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            df['processed_text'], df['Positive'],
            test_size=0.2, random_state=42, stratify=df['Positive']
        )

        # Vectorize text data
        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)

        # Train model
        self.model = LogisticRegression(random_state=42, max_iter=1000)
        self.model.fit(X_train_vec, y_train)

        # Make predictions
        y_pred = self.model.predict(X_test_vec)
        y_pred_proba = self.model.predict_proba(X_test_vec)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])

        self.model_metrics = {
            'model_name': 'Logistic Regression',
            'hyperparameters': {
                'max_iter': 1000,
                'random_state': 42,
                'solver': 'lbfgs'
            },
            'accuracy': accuracy,
            'auc_score': auc_score,
            'classification_report': classification_report(y_test, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'X_test': X_test,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'train_size': len(X_train),
            'test_size': len(X_test)
        }

        return self.model_metrics

    def predict(self, text):
        if self.model is None or self.vectorizer is None:
            return None, None

        processed_text = self.preprocess_text(text)
        text_vec = self.vectorizer.transform([processed_text])
        prediction = self.model.predict(text_vec)[0]
        probabilities = self.model.predict_proba(text_vec)[0]

        return prediction, probabilities

@st.cache_data
def load_data():
    try:
        df = pd.read_csv('amazon.csv')
        return df
    except FileNotFoundError:
        return None

def load_data_with_uploader():
    # Try to load from file first
    df = load_data()
    if df is not None:
        return df

    # If file not found, show uploader
    st.warning("amazon.csv file not found. Please upload your CSV file.")
    uploaded_file = st.file_uploader("Choose CSV file", type="csv")

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        # Validate columns
        if 'reviewText' in df.columns and 'Positive' in df.columns:
            st.success("File uploaded successfully!")
            return df
        else:
            st.error("CSV must contain 'reviewText' and 'Positive' columns.")
            return None

    return None

@st.cache_resource
def train_model(df):
    analyzer = SentimentAnalyzer()
    metrics = analyzer.train_model(df)
    return analyzer, metrics

def main():
    st.set_page_config(page_title="Sentiment Analysis Dashboard", layout="wide")
    st.title("ðŸŽ­ Sentiment Analysis Dashboard")

    # Load data
    df = load_data_with_uploader()
    if df is None:
        return

    # Train model
    with st.spinner("Training model... Please wait."):
        analyzer, metrics = train_model(df)

    # Create tabs
    tab1, tab2 = st.tabs(["ðŸ“Š Model Details", "ðŸ”® Prediction"])

    with tab1:
        st.header("Model Information")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("Model Details")
            st.write(f"**Model Name:** {metrics['model_name']}")
            st.write("**Hyperparameters:**")
            for param, value in metrics['hyperparameters'].items():
                st.write(f"  - {param}: {value}")

            st.subheader("Dataset Information")
            st.write(f"**Total Samples:** {len(df)}")
            st.write(f"**Training Samples:** {metrics['train_size']}")
            st.write(f"**Testing Samples:** {metrics['test_size']}")
            st.write(f"**Train/Test Split:** 80/20")

        with col2:
            st.subheader("Performance Metrics")
            st.metric("Accuracy", f"{metrics['accuracy']:.4f}")
            st.metric("AUC Score", f"{metrics['auc_score']:.4f}")

            # Classification report
            st.subheader("Classification Report")
            report_df = pd.DataFrame(metrics['classification_report']).transpose()
            st.dataframe(report_df.round(4))

        # Visualizations
        st.subheader("Model Performance Visualizations")

        col1, col2 = st.columns(2)

        with col1:
            # Confusion Matrix
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',
                       xticklabels=['Negative', 'Positive'],
                       yticklabels=['Negative', 'Positive'])
            plt.title('Confusion Matrix')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            st.pyplot(fig)

        with col2:
            # ROC Curve
            fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'][:, 1])
            fig, ax = plt.subplots(figsize=(6, 4))
            plt.plot(fpr, tpr, color='darkorange', lw=2,
                    label=f'ROC curve (AUC = {metrics["auc_score"]:.4f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic (ROC) Curve')
            plt.legend(loc="lower right")
            st.pyplot(fig)

        # Feature importance (top words)
        st.subheader("Top Important Features")
        feature_names = analyzer.vectorizer.get_feature_names_out()
        coefficients = analyzer.model.coef_[0]

        # Get top positive and negative features
        top_positive_idx = np.argsort(coefficients)[-10:][::-1]
        top_negative_idx = np.argsort(coefficients)[:10]

        col1, col2 = st.columns(2)

        with col1:
            st.write("**Top Positive Sentiment Words:**")
            for idx in top_positive_idx:
                st.write(f"- {feature_names[idx]}: {coefficients[idx]:.4f}")

        with col2:
            st.write("**Top Negative Sentiment Words:**")
            for idx in top_negative_idx:
                st.write(f"- {feature_names[idx]}: {coefficients[idx]:.4f}")

    with tab2:
        st.header("Text Sentiment Prediction")
        st.write("Enter any text below to analyze its sentiment:")

        # Text input
        user_text = st.text_area("Enter text for sentiment analysis:",
                                placeholder="Type or paste your text here...",
                                height=100)

        if st.button("Analyze Sentiment", type="primary"):
            if user_text.strip():
                prediction, probabilities = analyzer.predict(user_text)

                if prediction is not None:
                    # Display results
                    col1, col2 = st.columns(2)

                    with col1:
                        st.subheader("Prediction Result")
                        sentiment_label = "Positive" if prediction == 1 else "Negative"
                        sentiment_color = "green" if prediction == 1 else "red"
                        st.markdown(f"**Sentiment:** <span style='color: {sentiment_color}; font-size: 24px;'>{sentiment_label}</span>",
                                   unsafe_allow_html=True)

                    with col2:
                        st.subheader("Confidence Levels")
                        negative_conf = probabilities[0] * 100
                        positive_conf = probabilities[1] * 100

                        st.write(f"**Negative:** {negative_conf:.2f}%")
                        st.write(f"**Positive:** {positive_conf:.2f}%")

                        # Progress bars for confidence
                        st.progress(negative_conf/100, text=f"Negative: {negative_conf:.1f}%")
                        st.progress(positive_conf/100, text=f"Positive: {positive_conf:.1f}%")

                    # Confidence visualization
                    st.subheader("Confidence Visualization")
                    fig, ax = plt.subplots(figsize=(8, 4))
                    labels = ['Negative', 'Positive']
                    colors = ['red', 'green']
                    bars = ax.bar(labels, [negative_conf, positive_conf], color=colors, alpha=0.7)
                    ax.set_ylabel('Confidence (%)')
                    ax.set_title('Sentiment Confidence Levels')
                    ax.set_ylim(0, 100)

                    # Add value labels on bars
                    for bar in bars:
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                               f'{height:.1f}%', ha='center', va='bottom')

                    st.pyplot(fig)
                else:
                    st.error("Error in prediction. Please try again.")
            else:
                st.warning("Please enter some text to analyze.")

if __name__ == "__main__":
    main()